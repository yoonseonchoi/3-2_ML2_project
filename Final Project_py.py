# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1osTSRTRdpXRdUTCvDNnAjwDhkjxbx7xv
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torchvision import transforms, datasets
from torch.utils.data.sampler import SubsetRandomSampler

import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')
device = torch.device('cuda:0')

def normalization(dataset):
  mean = np.array([np.mean(x.numpy(), axis=(1,2)) for x, _ in dataset])
  r_mean = mean[:, 0].mean()
  g_mean = mean[:, 1].mean()
  b_mean = mean[:, 2].mean()
  
  std = np.array([np.std(x.numpy(), axis=(1,2)) for x, _ in dataset])
  r_std = std[:, 0].mean()
  g_std = std[:, 0].mean()
  b_std = std[:, 0].mean()

  return (r_mean, g_mean, b_mean), (r_std, g_std, b_std)

transform = transforms.Compose([transforms.ToTensor()])
trainset = datasets.CIFAR10(root ='./ data', train=True, download=True, transform=transform)

mean, std = normalization(trainset)
print('Mean (R, G, B): ', mean)
print('Standard deviation (R, G, B): ',std)

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])
transform_aug = transforms.Compose([transforms.Resize((32,32)), transforms.RandomHorizontalFlip(),
                                    transforms.RandomRotation(10), transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),
                                    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
                                    transforms.ToTensor(), transforms.Normalize(mean, std)])

trainset = datasets.CIFAR10(root ='./ data', train=True, download=True, transform=transform_aug)
valset = datasets.CIFAR10(root ='./ data', train=True, download=True, transform=transform)
testset = datasets.CIFAR10(root ='./ data', train=False, download=True, transform=transform)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

np.random.seed(0)
val_ratio = 0.1
train_size = len(trainset)
indices = list(range(train_size))
split_idx = int(np.floor(val_ratio*train_size))
np.random.shuffle(indices)
train_idx, val_idx = indices[split_idx:], indices[:split_idx]

train_sampler = SubsetRandomSampler(train_idx)
val_sampler = SubsetRandomSampler(val_idx)

batch_size = 128

train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler, num_workers=2)
val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, sampler=val_sampler, num_workers=2)
test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

def conv3(in_channels, out_channels, stride=1):
  return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)

class ResidualBlock(nn.Module):
  def __init__(self, in_channels, out_channels, stride=1, shortcut=None):
    super(ResidualBlock, self).__init__()
    self.conv1 = conv3(in_channels, out_channels, stride)
    self.bn1 = nn.BatchNorm2d(out_channels)
    self.relu = nn.ReLU(inplace=True)
    self.conv2 = conv3(out_channels, out_channels)
    self.bn2 = nn.BatchNorm2d(out_channels)
    self.shortcut = shortcut

  def forward(self, x):
    identity = x
    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu(out)
    out = self.conv2(out)
    out = self.bn2(out)
    if self.shortcut:
      identity = self.shortcut(x)
    out += identity
    out = self.relu(out)
    return out

class ResNet(nn.Module):
  def __init__(self, block, layers, num_classes=10):
    super(ResNet, self).__init__()
    self.in_channels = 16
    self.conv = conv3(3, self.in_channels)
    self.bn = nn.BatchNorm2d(self.in_channels)
    self.relu = nn.ReLU(inplace=True)
    self.layer1 = self.make_layer(block, 16, layers[0], 1)
    self.layer2 = self.make_layer(block, 32, layers[1], 2)
    self.layer3 = self.make_layer(block, 64, layers[2], 2)
    self.avgPool = nn.AvgPool2d(8)
    self.fc = nn.Linear(64, num_classes)

  def make_layer(self, block, out_channels, num_blocks, stride=1):
    shortcut = None
    if (stride != 1) or (self.in_channels != out_channels):
      shortcut = nn.Sequential(conv3(self.in_channels, out_channels, stride=stride),
                               nn.BatchNorm2d(out_channels))
    layers = []
    layers.append(block(self.in_channels, out_channels, stride, shortcut))
    self.in_channels = out_channels
    for _ in range(1, num_blocks):
      layers.append(block(out_channels, out_channels))
    return nn.Sequential(*layers)
    
  def forward(self, x):
    out = self.conv(x)
    out = self.bn(out)
    out = self.relu(out)
    out = self.layer1(out)
    out = self.layer2(out)
    out = self.layer3(out)
    out = self.avgPool(out)
    out = out.view(out.size(0), -1)
    out = self.fc(out)
    return out

model = ResNet(ResidualBlock, [7, 7, 7]).to(device)
loss_func = nn.CrossEntropyLoss().to(device)
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[32000, 48000], gamma=0.1)

epochs = 50
train_loss_list = []
val_loss_list = []
val_acc_list = []

for epoch in range(epochs):
  model.train()
  train_loss = 0
  train_total = 0
  for data, target in train_loader:
    data, target = data.to(device), target.to(device)
    scheduler.step()
    optimizer.zero_grad()
    output = model(data)
    loss = loss_func(output, target)
    loss.backward()
    optimizer.step()
    train_loss += loss.item()
    train_total += target.size(0)

  train_loss /= train_total
  train_loss_list.append(train_loss)
  
  model.eval()
  correct = 0
  val_total = 0
  val_loss = 0
  with torch.no_grad():
    for data, target in val_loader:
      data, target = data.to(device), target.to(device)
      output = model(data)
      val_loss += loss_func(output, target).item()
      _, prediction = torch.max(output.data, 1)
      val_total += target.size(0)
      correct += prediction.eq(target.view_as(prediction)).sum().item()

  val_loss /= val_total
  val_acc = 100.*correct/val_total

  val_loss_list.append(val_loss)
  val_acc_list.append(val_acc)
  
  print('Epoch: {}\t Train Loss: {:04f}\t Valid Loss: {:04f}\t Valid Accuracy: {:.2f}'.format(epoch+1, train_loss, val_loss, val_acc))

loss_list = torch.tensor(val_loss_list)
acc_list = torch.tensor(val_acc_list)

plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.plot(torch.tensor(range(1,51)), train_loss_list, label='train_loss')
plt.plot(torch.tensor(range(1,51)), val_loss_list, label='val_loss')
plt.legend()
plt.subplot(1,2,2)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.plot(torch.tensor(range(1,51)), val_acc_list)
plt.show()

epochs = 50
for epoch in range(epochs):
  model.eval()
  correct = 0
  total = 0
  with torch.no_grad():
    for data, target in test_loader:
      data, target = data.to(device), target.to(device)
      output = model(data)
      _, prediction = torch.max(output.data, 1)
      total += target.size(0)
      correct += prediction.eq(target.view_as(prediction)).sum().item()

  test_acc = 100.*correct/total
print('Test Accuracy: {:.2f}'.format(test_acc))